{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def makeDirectoryIfNeededAt(dir):\n",
    "        import os\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TODO: CAN-MIGRU Dataset - '1' is Attack, udl[i][j] == '1'\n",
    "#TODO: CAR-Hacking Dataset - 'R' is attack free, otherwise attack, udl[i][j] != 'R'\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    migru = 1\n",
    "    carHacking = 2\n",
    "        \n",
    "\n",
    "DATASET_TYPE = DatasetType.migru\n",
    "# DATASET_TYPE = DatasetType.carHacking\n",
    "\n",
    "#TODO: Change the files names\n",
    "# CAN-MIGRU Dataset \n",
    "#---------------------------------\n",
    "\n",
    "#Mixed\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Merged_Attack_and_AttackFree/Merged_ID_Attack_and_AttackFree.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/Merged_Attack_and_AttackFree/Merged_Label_Attack_and_AttackFree.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Merged_Attack_and_AttackFree/Attack_AttackFree_CAN_MIGRU_Files\"\n",
    "# filename = 'ClassifierResult_Attack_AttackFree_CAN_MIGRU.csv'\n",
    "\n",
    "#Injection\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/1.Merged_Injection_and_AttackFree/Merged_ID_Injection_and_AttackFree.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/1.Merged_Injection_and_AttackFree/Merged_Label_Injection_and_AttackFree.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/XAI_All/Injection\"\n",
    "# filename = 'ClassifierResult_Injection_AttackFree_CAN_MIGRU.csv'\n",
    "\n",
    "# # #Suspension\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/2.Merged_Suspension_and_AttackFree/Merged_ID_Suspension_and_AttackFree.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/2.Merged_Suspension_and_AttackFree/Merged_Label_Suspension_and_AttackFree.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/XAI_All/Suspension\"\n",
    "# filename = 'ClassifierResult_Suspension_AttackFree_CAN_MIGRU.csv'\n",
    "\n",
    "# Masquerade\n",
    "fileId = open('/Users/tamim028/github/CAN_RND/data/3.Merged_Masquerade_and_AttackFree/Merged_ID_Masquerade_and_AttackFree.txt', 'r') \n",
    "fileA = open('/Users/tamim028/github/CAN_RND/data/3.Merged_Masquerade_and_AttackFree/Merged_Label_Masquerade_and_AttackFree.txt','r') \n",
    "fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/XAI_All/Masquerade\"\n",
    "filename = 'ClassifierResult_Masquerade_AttackFree_CAN_MIGRU.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CarHackingDataset\n",
    "#---------------------------------\n",
    "#DoS\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/DosIDwithattackfree_car_hacking.txt', 'r')  \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/98Car hacing dataset processed/DosLabelwithattackfree_car_hacking.txt','r')  \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/DosAttackAndAttackFreeOutputFiles\"\n",
    "# filename = 'DoswithAttackfree_car_hacking.csv'\n",
    "\n",
    "# Fuzzy\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/fuzzyIDwithattackfree_car_hacking.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/fuzzyLabelwithattackfree_car_hacking.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/FuzzyAttack_and_AttackFree_OutputFiles\"\n",
    "# filename = 'Fuzzy_with_Attackfree_car_hacking.csv'\n",
    "\n",
    "# Gear\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/gearIDwithattackfree_car_hacking.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/gearLabelwithattackfree_car_hacking.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/GearAttack_and_AttackFree_OutputFiles\"\n",
    "# filename = 'Gear_with_Attackfree_car_hacking.csv'\n",
    "\n",
    "# Rpm\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/rpmIDwithattackfree_car_hacking.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/rpmLabelwithattackfree_car_hacking.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/RpmAttack_and_AttackFree_OutputFiles\"\n",
    "# filename = 'Rpm_with_Attackfree_car_hacking.csv'\n",
    "\n",
    "# #Mix\n",
    "# fileId = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/allIDwithattackfree_car_hacking.txt', 'r') \n",
    "# fileA = open('/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/allLabelwithattackfree_car_hacking.txt','r') \n",
    "# fileFolderDirectory = \"/Users/tamim028/github/CAN_RND/data/Car hacing dataset processed/MixedAttack_and_AttackFree_OutputFiles\"\n",
    "# filename = 'Mixed_with_Attackfree_car_hacking.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Event: Current DatasetType is {DATASET_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "heyThere = 0\n",
    "impersion_variable = False\n",
    "\n",
    "detectList = []\n",
    "counter = 1\n",
    "graphList = []\n",
    "ultimateGraphList = []\n",
    "utlimateaDetectionList = []\n",
    "graphCounter = 0\n",
    "ll = []\n",
    "\n",
    "messageSize = 200\n",
    "\n",
    "for line in fileId:\n",
    "  node = line.split()[0]\n",
    "\n",
    "  graphList.append(node)\n",
    "\n",
    "  if len(graphList) == messageSize:\n",
    "    ultimateGraphList.append(graphList)\n",
    "    graphList = []\n",
    "\n",
    "\n",
    "ugl1 = ultimateGraphList\n",
    "\n",
    "for line in fileA:\n",
    "  attackChecking = line.split()[0]\n",
    "  ll.append(attackChecking)\n",
    "\n",
    "  detectList.append(attackChecking)\n",
    "\n",
    "  if len(detectList) == messageSize:\n",
    "    utlimateaDetectionList.append(detectList)\n",
    "    detectList = []\n",
    "\n",
    "udl1 = utlimateaDetectionList\n",
    "\n",
    "print(len(ugl1))\n",
    "print(len(udl1))\n",
    "\n",
    "print(ugl1[0])\n",
    "print(len(ugl1[0]))\n",
    "print(len(udl1[0]))\n",
    "print(udl1[0])\n",
    "print(udl1[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(udl1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utlimateaDetectionList[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# import torch\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "def getPrClosBewtenIndegcenOutdegcen(uList,vList,heyThere):\n",
    "\n",
    "    G_for_new_added_features = nx.DiGraph()  # Create a directed graph\n",
    "\n",
    "    edges = [(u, v) for u, v in zip(uList, vList)]\n",
    "\n",
    "    G_for_new_added_features.add_edges_from(edges)\n",
    "\n",
    "    pr = nx.pagerank(G_for_new_added_features, alpha = 0.8)\n",
    "    closeness_centrality = nx.closeness_centrality(G_for_new_added_features)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G_for_new_added_features)\n",
    "    in_deg_centrality = nx.in_degree_centrality(G_for_new_added_features) \n",
    "    out_deg_centrality = nx.out_degree_centrality(G_for_new_added_features) \n",
    "  \n",
    "    pr = dict(sorted(pr.items(), reverse=False))\n",
    "    closeness_centrality = dict(sorted(closeness_centrality.items(), reverse=False))\n",
    "                                \n",
    "    betweenness_centrality = dict(sorted(betweenness_centrality.items(), reverse=False))\n",
    "    in_deg_centrality = dict(sorted(in_deg_centrality.items(), reverse=False))\n",
    "    out_deg_centrality = dict(sorted(out_deg_centrality.items(), reverse=False))\n",
    "\n",
    "    pr = np.mean(list(pr.values()))\n",
    "    closeness_centrality = np.mean(list(closeness_centrality.values()))\n",
    "    betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "    in_deg_centrality = np.mean(list(in_deg_centrality.values()))\n",
    "    out_deg_centrality = np.mean(list(out_deg_centrality.values()))\n",
    "\n",
    "\n",
    "    return pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugl,udl = ugl1,udl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(np.array(udl).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making graph from the 200 sequential of id and store them in a file as Graph adjacent list\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "# import torch\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from torch_geometric.data import Data, DataLoader\n",
    "gAttackfree=nx.DiGraph()\n",
    "gAttacked = nx.DiGraph()\n",
    "\n",
    "lenugl = len(ugl)\n",
    "\n",
    "counter = 0\n",
    "gAttackfreeCounter = 0\n",
    "gAttackedCounter = 0\n",
    "\n",
    "allLabelList = []\n",
    "graphFeatureList_5 = []\n",
    "graphFeatureList_2 = []\n",
    "uvList = []\n",
    "DataList = []\n",
    "\n",
    "ultimateUList = []\n",
    "ultimateVList = []\n",
    "\n",
    "attackFreeWindowCount = lenugl\n",
    "for i in range(0,lenugl):\n",
    "  uList = []\n",
    "  vList = []\n",
    "  attackFree = 1\n",
    "  fList = []\n",
    "  all_unique_nodes = list(set(ugl[i]))\n",
    "  ln = len(all_unique_nodes)\n",
    "  array = np.zeros((ln,ln))\n",
    "  for j in range (0, len(ugl[i])-1):\n",
    "     u = all_unique_nodes.index(ugl[i][j])\n",
    "     v = all_unique_nodes.index(ugl[i][j+1])\n",
    "     uList.append(u)\n",
    "     vList.append(v)\n",
    "   \n",
    "  uvList.append([uList,vList])\n",
    "  ultimateUList.append(uList)\n",
    "  ultimateVList.append(vList)\n",
    "  \n",
    "  heyThere = heyThere + 1\n",
    "  pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality = getPrClosBewtenIndegcenOutdegcen(uList,vList,heyThere)\n",
    "\n",
    "  # indeg, outdeg, pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality\n",
    "\n",
    "\n",
    "  graphFeatureList_5.append([pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality])\n",
    "  graphFeatureList_2.append([in_deg_centrality, out_deg_centrality])\n",
    "\n",
    "  for j in range (0,len(ugl[i])):\n",
    "    if DATASET_TYPE == DatasetType.migru:\n",
    "      if udl[i][j] == '1':\n",
    "        attackFree = 0\n",
    "        attackFreeWindowCount = attackFreeWindowCount - 1\n",
    "        break\n",
    "    else:\n",
    "      if udl[i][j] != 'R':\n",
    "        attackFree = 0\n",
    "        attackFreeWindowCount = attackFreeWindowCount - 1\n",
    "        break\n",
    "\n",
    "  y = attackFree\n",
    "  allLabelList.append(attackFree)\n",
    "\n",
    "\n",
    "  # if impersion_variable == True and y == 0:\n",
    "  #   for m in range(30):\n",
    "  #     graphFeatureList_5.append([pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality])\n",
    "  #     graphFeatureList_2.append([in_deg_centrality, out_deg_centrality])\n",
    "  #     allLabelList.append(attackFree)\n",
    "\n",
    "\n",
    "\n",
    "print(len(graphFeatureList_5))\n",
    "print(len(graphFeatureList_2))\n",
    "print(len(allLabelList))\n",
    "print(allLabelList.count(0))\n",
    "print(allLabelList.count(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Attack Free Windows Count: {attackFreeWindowCount} and Attack Windows Count: {lenugl - attackFreeWindowCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = set(allLabelList)  # Find unique classes\n",
    "number_of_classes = len(unique_classes)  # Count the unique classes\n",
    "print(f\"Number of classes: {number_of_classes}\")\n",
    "print(f\"Unique classes: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import csv\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "class GraphFeatureType(Enum):\n",
    "        two = 1\n",
    "        five = 2\n",
    "\n",
    "class ModelGenerator:\n",
    "    \n",
    "    def __init__(self, graphFeatureType: GraphFeatureType, graphFeatureList, allLabelList, modelSaveDir, resultFileName: str = \"\"):\n",
    "        self.graphFeatureType = graphFeatureType\n",
    "        self.graphFeatureList = graphFeatureList\n",
    "        self.allLabelList = allLabelList\n",
    "        self.modelSaveDir = modelSaveDir\n",
    "        if resultFileName == \"\":\n",
    "            if self.graphFeatureType == GraphFeatureType.two:\n",
    "                self.resultFileName = \"Classifier_2F_Result.csv\"\n",
    "            else:\n",
    "                self.resultFileName = \"Classifier_2F_Result.csv\"\n",
    "        else:\n",
    "            self.resultFileName = resultFileName\n",
    "        \n",
    "    def generateResultsForFeature(self, fullDatasetOnly: bool = True):\n",
    "        model_save_dir = self.modelSaveDir\n",
    "\n",
    "        # Split the data into training and testing sets (70% training, 30% testing)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.graphFeatureList, self.allLabelList, test_size=0.3, random_state=42)\n",
    "        \n",
    "        classifiers = {\n",
    "            'GaussianNB': GaussianNB(),\n",
    "            'MultinomialNB': MultinomialNB(),\n",
    "            'ComplementNB': ComplementNB(),\n",
    "            'LDA': LinearDiscriminantAnalysis(),\n",
    "            'DecisionTree': DecisionTreeClassifier(),\n",
    "            'GradientBoosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        }\n",
    "\n",
    "        # Open a file to write the results\n",
    "        with open(self.resultFileName, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the header\n",
    "            writer.writerow(['Percentage', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Training Time (s)', 'Testing Time (s)'])\n",
    "\n",
    "            if fullDatasetOnly == False:\n",
    "                for test_size_values in np.arange(0.3, 1.1, 0.3):\n",
    "                    X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_train, y_train, train_size=test_size_values, random_state=42)\n",
    "\n",
    "                    print('Percentages:', test_size_values * 100)\n",
    "                    print('Count of class 1 in y_test:', y_test.count(1))\n",
    "                    print('Count of class 0 in y_test:', y_test.count(0))\n",
    "                    print('Length of y_test:', len(y_test))\n",
    "\n",
    "                    for name, clf in classifiers.items():\n",
    "                        # Train and evaluate the classifier\n",
    "                        start_time = time.time()\n",
    "                        clf.fit(X_train_p, y_train_p)\n",
    "                        end_time = time.time()\n",
    "                        training_time = end_time - start_time\n",
    "\n",
    "                        start_time = time.time()\n",
    "                        predictions = clf.predict(X_test)\n",
    "                        end_time = time.time()\n",
    "                        testing_time = end_time - start_time\n",
    "\n",
    "                        accuracy = accuracy_score(y_test, predictions)\n",
    "                        confusion = confusion_matrix(y_test, predictions)\n",
    "                        confusion[0, 0], confusion[1, 1] = confusion[1, 1], confusion[0, 0]  # Swap TN and TP\n",
    "\n",
    "                        precision = precision_score(y_test, predictions)\n",
    "                        recall = recall_score(y_test, predictions)\n",
    "                        f1 = f1_score(y_test, predictions)\n",
    "\n",
    "                        print(f'\\n{name} Results:')\n",
    "                        print(f\"Accuracy: {accuracy}\")\n",
    "                        print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "                        print(f\"Precision: {precision}\")\n",
    "                        print(f\"Recall: {recall}\")\n",
    "                        print(f\"F1 Score: {f1}\")\n",
    "                        print(f\"Training Time: {training_time} seconds\")\n",
    "                        print(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "                        # Write the results to the file\n",
    "                        writer.writerow([test_size_values * 100, name, accuracy, precision, recall, f1,training_time, testing_time])\n",
    "            \n",
    "            # Evaluate classifiers with the entire training set\n",
    "            print('Percentages 100%')\n",
    "            X_train_p, y_train_p = X_train, y_train\n",
    "\n",
    "            print('Count of class 1 in y_test:', y_test.count(1))\n",
    "            print('Count of class 0 in y_test:', y_test.count(0))\n",
    "            print('Length of y_test:', len(y_test))\n",
    "\n",
    "            for name, clf in classifiers.items():\n",
    "                start_time = time.time()\n",
    "                clf.fit(X_train_p, y_train_p)\n",
    "                end_time = time.time()\n",
    "                training_time = end_time - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                predictions = clf.predict(X_test)\n",
    "                end_time = time.time()\n",
    "                testing_time = end_time - start_time\n",
    "\n",
    "                model_filename = f\"{model_save_dir}/100_{name}_model.pkl\"\n",
    "                joblib.dump(clf, model_filename)\n",
    "                print(f\"Entire data - Model saved for {name}: {model_filename}\")\n",
    "\n",
    "                # Save X_test for this classifier\n",
    "                x_test_filename = f\"{model_save_dir}/100_{name}_X_test.npy\"\n",
    "                X_test_to_save = np.array(X_test)\n",
    "                np.save(x_test_filename, np.array(X_test_to_save))\n",
    "                print(f\"X_test saved for {name}: {x_test_filename}\")\n",
    "\n",
    "                accuracy = accuracy_score(y_test, predictions)\n",
    "                confusion = confusion_matrix(y_test, predictions)\n",
    "                confusion[0, 0], confusion[1, 1] = confusion[1, 1], confusion[0, 0]  # Swap TN and TP\n",
    "\n",
    "                precision = precision_score(y_test, predictions)\n",
    "                recall = recall_score(y_test, predictions)\n",
    "                f1 = f1_score(y_test, predictions)\n",
    "\n",
    "                print(f'\\n{name} Results:')\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"F1 Score: {f1}\")\n",
    "                print(f\"Training Time: {training_time} seconds\")\n",
    "                print(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "                # Write the results to the file\n",
    "                writer.writerow([100, name, accuracy, precision, recall, f1, training_time, testing_time])\n",
    "    \n",
    "    def getFeatureNames(self):\n",
    "        if self.graphFeatureType == GraphFeatureType.five:\n",
    "            return [\n",
    "                \"PR\",\n",
    "                \"CC\",\n",
    "                \"BC\",\n",
    "                \"IDC\",\n",
    "                \"ODC\"\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                \"IDC\",\n",
    "                \"ODC\"\n",
    "            ]\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_save_dir_5F = f\"{fileFolderDirectory}/5F_Files\"\n",
    "model_save_dir_2F = f\"{fileFolderDirectory}/2F_Files\"\n",
    "\n",
    "print(model_save_dir_5F)\n",
    "\n",
    "Utils.makeDirectoryIfNeededAt(model_save_dir_2F)\n",
    "Utils.makeDirectoryIfNeededAt(model_save_dir_5F)\n",
    "\n",
    "modelGenerator5F = ModelGenerator(graphFeatureType=GraphFeatureType.five, \n",
    "                                  graphFeatureList=graphFeatureList_5, \n",
    "                                  allLabelList=allLabelList, \n",
    "                                  modelSaveDir=model_save_dir_5F,\n",
    "                                  resultFileName=filename\n",
    "                                  )\n",
    "modelGenerator2F = ModelGenerator(graphFeatureType=GraphFeatureType.two, \n",
    "                                  graphFeatureList=graphFeatureList_2, \n",
    "                                  allLabelList=allLabelList, \n",
    "                                  modelSaveDir=model_save_dir_2F,\n",
    "                                  resultFileName=filename\n",
    "                                  )\n",
    "\n",
    "\n",
    "modelGenerator5F.generateResultsForFeature(fullDatasetOnly=True)\n",
    "modelGenerator2F.generateResultsForFeature(fullDatasetOnly=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For XAI - SHAP\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class SHAPGenerator:\n",
    "    def __init__(self, fileDirectory, feature_names):\n",
    "        self.fileDirectory = fileDirectory\n",
    "        self.feature_names = feature_names\n",
    "        self.ultimateShapValues = []\n",
    "        self.modelNames = []\n",
    "        self.ultimateX_Tests = []\n",
    "\n",
    "# Get all model and npy file paths\n",
    "\n",
    "    def generateSHAPValueForAllModel(self):\n",
    "        model_files = glob.glob(os.path.join(self.fileDirectory, \"*.pkl\"))\n",
    "        npy_files = {os.path.splitext(os.path.basename(f))[0]: f for f in glob.glob(os.path.join(self.fileDirectory, \"*.npy\"))}\n",
    "\n",
    "        # SHAP analysis for each model\n",
    "\n",
    "        for model_path in model_files:\n",
    "            # Extract model name without extension\n",
    "            model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "            print(f\"model_name - {model_name}\")\n",
    "            # Check if corresponding .npy file exists\n",
    "            testFileName = model_name.replace(\"_model\", \"_X_test\")\n",
    "            print(f\"TEST: {testFileName}\")\n",
    "            if testFileName in npy_files:\n",
    "                print()\n",
    "                npy_path = npy_files[testFileName]\n",
    "                \n",
    "                # Load model and X_test\n",
    "                print(f\"Loading model: {model_path}\")\n",
    "                model = joblib.load(model_path)\n",
    "                \n",
    "                print(f\"Loading X_test: {npy_path}\")\n",
    "                X_test = np.load(npy_path)\n",
    "                \n",
    "                # Determine appropriate feature names\n",
    "                if len(self.feature_names) == 0:\n",
    "                    names = [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
    "                    self.feature_names = names\n",
    "                \n",
    "                explainer = shap.Explainer(model.predict, X_test)  # `voting_regressor.predict` is callable\n",
    "                shap_values = explainer(X_test)\n",
    "                \n",
    "                self.ultimateX_Tests.append(X_test)\n",
    "                self.ultimateShapValues.append(shap_values)\n",
    "                self.modelNames.append(model_name)\n",
    "                print(f\"Event: Added {model_name} SHAP Values\")\n",
    "            else:\n",
    "                print(f\"No corresponding X_test found for model: {model_name}\")\n",
    "\n",
    "    def applySHAP(self):\n",
    "        \n",
    "        modelsCnt = len(self.modelNames)\n",
    "        print(f\"Lenth of models cnt: {modelsCnt} \")\n",
    "        for i in range(0, modelsCnt):\n",
    "            model_name = self.modelNames[i]\n",
    "            shap_values = self.ultimateShapValues[i]\n",
    "            X_test = self.ultimateX_Tests[i]\n",
    "            # Generate and save SHAP summary & WaterFall plot            \n",
    "            self._private_summaryPlot(shap_values=shap_values, X_test=X_test, model_name=model_name, feature_names=self.feature_names) #okay\n",
    "            self._private_waterFallPlot(shap_values=shap_values, model_name=model_name, feature_names=self.feature_names)      \n",
    "            \n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _private_summaryPlot(self, shap_values, X_test, model_name, feature_names):\n",
    "    \n",
    "    destinationFolder = f\"{self.fileDirectory}/SummaryPlot\"\n",
    "    Utils.makeDirectoryIfNeededAt(destinationFolder)\n",
    "      \n",
    "    shap_summary_plot_path = os.path.join(destinationFolder, f\"{model_name}_summary.png\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))  # Set the figure size before plotting\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, model_name = , show=False)  # Set show=False to avoid immediate display\n",
    "    plt.tight_layout()  # Ensure the layout doesn't cut off content\n",
    "    plt.savefig(shap_summary_plot_path, bbox_inches='tight')  # Use bbox_inches='tight' to ensure everything fits\n",
    "    plt.close()  # Close the plot to avoid overwriting\n",
    "    print(f\"SHAP summary plot saved at: {shap_summary_plot_path}\")\n",
    "    \n",
    "def _private_waterFallPlot(self, shap_values, model_name, feature_names):\n",
    "    # Works Fine......\n",
    "    # Generate and save SHAP waterfall plot\n",
    "    print(\"Generating SHAP waterfall plot...\")\n",
    "    destinationFolder = f\"{self.fileDirectory}/WaterFall\"\n",
    "    Utils.makeDirectoryIfNeededAt(destinationFolder)\n",
    "    \n",
    "    shap_waterfall_plot_path = os.path.join(destinationFolder, f\"{model_name}_waterfall_summary.png\")\n",
    "\n",
    "    instance_index = 0  # For the first instance\n",
    "    shap_values_with_names = shap.Explanation(\n",
    "        values=shap_values.values,        # SHAP values\n",
    "        base_values=shap_values.base_values,  # Base values\n",
    "        feature_names=feature_names       # Feature names\n",
    "    )\n",
    "\n",
    "    #option Workable\n",
    "    shap.plots.waterfall(shap_values_with_names[instance_index], show=False)\n",
    "    fig = plt.gcf()  # Get the current figure after the summary plot has been created\n",
    "    fig.set_size_inches(10, 6)  # Set custom size (5 inches by 3 inches)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_waterfall_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"SHAP waterfall plot saved at: {shap_waterfall_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySHAP(self):\n",
    "    \n",
    "    modelsCnt = len(self.modelNames)\n",
    "    print(f\"Lenth of models cnt: {modelsCnt} \")\n",
    "    for i in range(0, modelsCnt):\n",
    "        model_name = self.modelNames[i]\n",
    "        shap_values = self.ultimateShapValues[i]\n",
    "        X_test = self.ultimateX_Tests[i]\n",
    "        # Generate and save SHAP summary & WaterFall plot            \n",
    "        self._private_summaryPlot(shap_values=shap_values, X_test=X_test, model_name=model_name, feature_names=self.feature_names) #okay\n",
    "        self._private_waterFallPlot(shap_values=shap_values, model_name=model_name, feature_names=self.feature_names)\n",
    "        \n",
    "SHAPGenerator._private_summaryPlot = _private_summaryPlot \n",
    "SHAPGenerator._private_waterFallPlot = _private_waterFallPlot       \n",
    "SHAPGenerator.applySHAP = applySHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames_5F = modelGenerator5F.getFeatureNames()\n",
    "print(featureNames_5F)\n",
    "shapGenerator_5F = SHAPGenerator(model_save_dir_5F, featureNames_5F)\n",
    "shapGenerator_5F.generateSHAPValueForAllModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapGenerator_5F.applySHAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames_2F = modelGenerator2F.getFeatureNames()\n",
    "print(featureNames_2F)\n",
    "shapGenerator_2F = SHAPGenerator(model_save_dir_2F, featureNames_2F)\n",
    "shapGenerator_2F.generateSHAPValueForAllModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapGenerator_2F.applySHAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional..........\n",
    "#Play Sound When Running Complete....\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "file_path = '/Users/tamim028/github/CAN_RND/src/file_example_WAV_1MG.wav'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File found: {file_path}\")\n",
    "    rate, data = wav.read(file_path)\n",
    "    sd.play(data, rate)\n",
    "    sd.wait()  # Wait until sound has finished playing\n",
    "else:\n",
    "    print(f\"File not found at: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

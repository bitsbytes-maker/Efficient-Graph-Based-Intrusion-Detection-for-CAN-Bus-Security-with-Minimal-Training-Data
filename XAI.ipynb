{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def makeDirectoryIfNeededAt(dir):\n",
    "        import os\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"/Users/tamim028/github/CAN_RND/data/XAI_All_temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved .npy files back into the same format\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# model_save_dir = \"/Users/tamim028/github/CAN_RND/data/Saved_Models/2F_models_and_files\"\n",
    "\n",
    "# # graphFeatureList_5 = np.load(os.path.join(save_dir, \"graphFeatureList_5.npy\"), allow_pickle=True).tolist()\n",
    "# graphFeatureList_2 = np.load(os.path.join(model_save_dir, \"graphFeatureList_2.npy\"), allow_pickle=True).tolist()\n",
    "# allLabelList = np.load(os.path.join(model_save_dir, \"allLabelList.npy\"), allow_pickle=True).tolist()\n",
    "\n",
    "# # Print the length and content to verify\n",
    "# # print(len(graphFeatureList_5))\n",
    "# print(len(graphFeatureList_2))\n",
    "# print(len(allLabelList))\n",
    "# print(allLabelList.count(0))\n",
    "# print(allLabelList.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "# import time\n",
    "# import csv\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load your dataset or create a sample dataset\n",
    "# # X, y = load_your_data()\n",
    "\n",
    "# # Assuming you have already loaded your data into graphFeatureList_5 and allLabelList\n",
    "# # graphFeatureList_5, allLabelList = load_your_data()\n",
    "\n",
    "# # Split the data into training and testing sets (70% training, 30% testing)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(graphFeatureList_5, allLabelList, test_size=0.3, random_state=42) #5 features\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(graphFeatureList_2, allLabelList, test_size=0.3, random_state=42) #2 features\n",
    "# # model_save_dir = \"/Users/tamim028/github/CAN_RND/data/Saved_Models/2F_models_and_files\"\n",
    "\n",
    "# classifiers = {\n",
    "#     'GaussianNB': GaussianNB(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'ComplementNB': ComplementNB(),\n",
    "#     'LDA': LinearDiscriminantAnalysis(),\n",
    "#     'DecisionTree': DecisionTreeClassifier(),\n",
    "#     'GradientBoosting': GradientBoostingClassifier(),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "# }\n",
    "# filename = 'classifier_results_attack_with_attackFree.csv'\n",
    "# # Open a file to write the results\n",
    "# with open(filename, 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     # Write the header\n",
    "#     writer.writerow(['Percentage', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Training Time (s)', 'Testing Time (s)'])\n",
    "\n",
    "#     # Evaluate classifiers with the entire training set\n",
    "#     print('Percentages 100%')\n",
    "#     X_train_p, y_train_p = X_train, y_train\n",
    "\n",
    "#     print('Count of class 1 in y_test:', y_test.count(1))\n",
    "#     print('Count of class 0 in y_test:', y_test.count(0))\n",
    "#     print('Length of y_test:', len(y_test))\n",
    "\n",
    "#     for name, clf in classifiers.items():\n",
    "#         start_time = time.time()\n",
    "#         clf.fit(X_train_p, y_train_p)\n",
    "#         end_time = time.time()\n",
    "#         training_time = end_time - start_time\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         predictions = clf.predict(X_test)\n",
    "#         end_time = time.time()\n",
    "#         testing_time = end_time - start_time\n",
    "\n",
    "#         model_filename = f\"{model_save_dir}/100_{name}_model.pkl\"\n",
    "#         joblib.dump(clf, model_filename)\n",
    "#         print(f\"Entire data - Model saved for {name}: {model_filename}\")\n",
    "\n",
    "#          # Save X_test for this classifier\n",
    "#         x_test_filename = f\"{model_save_dir}/100_{name}_X_test.npy\"\n",
    "#         X_test_to_save = np.array(X_test)\n",
    "#         np.save(x_test_filename, np.array(X_test_to_save))\n",
    "#         print(f\"X_test saved for {name}: {x_test_filename}\")\n",
    "\n",
    "#         accuracy = accuracy_score(y_test, predictions)\n",
    "#         confusion = confusion_matrix(y_test, predictions)\n",
    "#         confusion[0, 0], confusion[1, 1] = confusion[1, 1], confusion[0, 0]  # Swap TN and TP\n",
    "\n",
    "#         precision = precision_score(y_test, predictions)\n",
    "#         recall = recall_score(y_test, predictions)\n",
    "#         f1 = f1_score(y_test, predictions)\n",
    "\n",
    "#         print(f'\\n{name} Results:')\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "#         print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "#         print(f\"Precision: {precision}\")\n",
    "#         print(f\"Recall: {recall}\")\n",
    "#         print(f\"F1 Score: {f1}\")\n",
    "#         print(f\"Training Time: {training_time} seconds\")\n",
    "#         print(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "#         # Write the results to the file\n",
    "#         writer.writerow([100, name, accuracy, precision, recall, f1, training_time, testing_time])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr, closeness_centrality, betweenness_centrality, in_deg_centrality, out_deg_centrality\n",
    "# #large names\n",
    "# feature_names_graphFeatureList_5 = [\n",
    "#     \"PageRank\",\n",
    "#     \"Closeness Centrality\",\n",
    "#     \"Betweenness Centrality\",\n",
    "#     \"In-Degree Centrality\",\n",
    "#     \"Out-Degree Centrality\"\n",
    "# ]\n",
    "\n",
    "# feature_names_graphFeatureList_2 = [\n",
    "#     \"In-Degree Centrality\",\n",
    "#     \"Out-Degree Centrality\"\n",
    "# ]\n",
    "\n",
    "#short feature bname\n",
    "feature_names_graphFeatureList_5 = [\n",
    "    \"PR\",\n",
    "    \"CC\",\n",
    "    \"BC\",\n",
    "    \"IDC\",\n",
    "    \"ODC\"\n",
    "]\n",
    "\n",
    "feature_names_graphFeatureList_2 = [\n",
    "    \"IDC\",\n",
    "    \"ODC\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For XAI - SHAP\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder paths\n",
    "model_folder = model_save_dir\n",
    "npy_folder = model_save_dir\n",
    "\n",
    "# Get all model and npy file paths\n",
    "model_files = glob.glob(os.path.join(model_folder, \"*.pkl\"))\n",
    "npy_files = {os.path.splitext(os.path.basename(f))[0]: f for f in glob.glob(os.path.join(npy_folder, \"*.npy\"))}\n",
    "\n",
    "# SHAP analysis for each model\n",
    "ultimateShapValues = []\n",
    "ultimateX_test = []\n",
    "modelNames = []\n",
    "ultimateModelList = []\n",
    "ultimateModelListNames = []\n",
    "for model_path in model_files:\n",
    "    # Extract model name without extension\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    print(f\"model_name - {model_name}\")\n",
    "    # Check if corresponding .npy file exists\n",
    "    testFileName = model_name.replace(\"_model\", \"_X_test\")\n",
    "    print(f\"TEST: {testFileName}\")\n",
    "    if testFileName in npy_files:\n",
    "        print()\n",
    "        npy_path = npy_files[testFileName]\n",
    "        \n",
    "        # Load model and X_test\n",
    "        print(f\"Loading model: {model_path}\")\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        print(f\"Loading X_test: {npy_path}\")\n",
    "        X_test = np.load(npy_path)\n",
    "        \n",
    "         # Determine appropriate feature names\n",
    "        if X_test.shape[1] == len(feature_names_graphFeatureList_5):\n",
    "            feature_names = feature_names_graphFeatureList_5\n",
    "        elif X_test.shape[1] == len(feature_names_graphFeatureList_2):\n",
    "            feature_names = feature_names_graphFeatureList_2\n",
    "        else:\n",
    "            print(\"Event: default feature names.\")\n",
    "            feature_names = [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
    "        \n",
    "        \n",
    "        explainer = shap.Explainer(model.predict, X_test)  # `voting_regressor.predict` is callable\n",
    "        shap_values = explainer(X_test)\n",
    "\n",
    "        ultimateShapValues.append(shap_values)\n",
    "        ultimateModelList.append(model)\n",
    "        ultimateModelListNames.append(model_name)\n",
    "        ultimateX_test.append(X_test)\n",
    "        modelNames.append(model_name)\n",
    "        \n",
    "        print(f\"Event: Added {model_name} SHAP Values\")\n",
    "    else:\n",
    "        print(f\"No corresponding X_test found for model: {model_name}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define a dictionary to store all the data\n",
    "data_to_save = {\n",
    "    \"ultimateShapValues\": ultimateShapValues,\n",
    "    \"ultimateModelList\": ultimateModelList,\n",
    "    \"ultimateModelListNames\": ultimateModelListNames,\n",
    "    \"ultimateX_test\": ultimateX_test,\n",
    "}\n",
    "\n",
    "uFileName = \"ultimateDataFile.pkl\"\n",
    "ultimateDataFileSaveDir = f\"{model_save_dir}/DataFile/\"\n",
    "Utils.makeDirectoryIfNeededAt(ultimateDataFileSaveDir)\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open(f\"{ultimateDataFileSaveDir}/{uFileName}\", \"wb\") as file:\n",
    "    pickle.dump(data_to_save, file)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the file\n",
    "with open(f\"{ultimateDataFileSaveDir}/{uFileName}\", \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "# Extract the lists from the loaded dictionary\n",
    "ultimateShapValues = loaded_data[\"ultimateShapValues\"]\n",
    "ultimateModelList = loaded_data[\"ultimateModelList\"]\n",
    "ultimateModelListNames = loaded_data[\"ultimateModelListNames\"]\n",
    "ultimateX_test = loaded_data[\"ultimateX_test\"]\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Suspension_100_XGBoost_model', 'Injection_100_XGBoost_model', 'Masquerade_100_XGBoost_model', 'Mixed_100_XGBoost_model']\n"
     ]
    }
   ],
   "source": [
    "print(ultimateModelListNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SuspensionAF', 'InjectionAF', 'MasqueradeAF', 'MixedAF']\n"
     ]
    }
   ],
   "source": [
    "print(modelNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"SuspensionAF\",\n",
    "              \"InjectionAF\",\n",
    "              \"MasqueradeAF\",\n",
    "              \"MixedAF\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Shap Values to folder for each model.\n",
    "\n",
    "def _private_summaryPlot(save_dir, shap_values, X_test, model_name, feature_names):\n",
    "    \n",
    "    destinationFolder = f\"{save_dir}/{model_name}/SummaryPlot\"\n",
    "    Utils.makeDirectoryIfNeededAt(destinationFolder)\n",
    "      \n",
    "    shap_summary_plot_path = os.path.join(destinationFolder, f\"{model_name}_summary.png\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))  # Set the figure size before plotting\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)  # Set show=False to avoid immediate display\n",
    "    plt.tight_layout()  # Ensure the layout doesn't cut off content\n",
    "    plt.title(f\"SHAP Summary Plot - Model: {model_name}\")\n",
    "    plt.savefig(shap_summary_plot_path, bbox_inches='tight')  # Use bbox_inches='tight' to ensure everything fits\n",
    "    plt.close()  # Close the plot to avoid overwriting\n",
    "    print(f\"SHAP summary plot saved at: {shap_summary_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Lenth of ShapValuesArr: 4 \n",
      "Event: lencheck shap - 164184 and lenX = 164184\n",
      "Event: lencheck shap - 182205 and lenX = 182205\n",
      "Event: lencheck shap - 163046 and lenX = 163046\n",
      "Event: lencheck shap - 189995 and lenX = 189995\n"
     ]
    }
   ],
   "source": [
    "print(len(modelNames))\n",
    "sz = len(modelNames)\n",
    "print(f\"Lenth of ShapValuesArr: {len(ultimateShapValues)} \")\n",
    "for i in range(0, sz):\n",
    "    print(f\"Event: lencheck shap - {len(ultimateShapValues[i])} and lenX = {len(ultimateX_test[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(0, sz):\n",
    "    model_name = modelNames[i]\n",
    "    shap_values = ultimateShapValues[i]\n",
    "    X_test = ultimateX_test[i]\n",
    "    \n",
    "    # Generate and save SHAP summary plot\n",
    "    shap_summary_plot_path = os.path.join(model_folder, f\"{model_name}_shap_summary.png\")\n",
    "    print(f\"EVENT: summaryPlotting for name: {model_name} and featureNames: {feature_names} lenSHAP = {len(shap_values)} lenX = {len(ultimateX_test)}\")\n",
    "    \n",
    "    _private_summaryPlot(save_dir=model_save_dir,shap_values=shap_values, X_test=X_test, model_name=model_name, feature_names=feature_names)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waterfall for all models\n",
    "for i in range(0, len(ultimateModelList)):\n",
    "    model_name = modelNames[i]\n",
    "    shap_values = ultimateShapValues[i]\n",
    "    X_test = ultimateX_test[i]\n",
    "    model = ultimateModelList[i]\n",
    "    print(f\"Event: MODEL NAME: {model_name} and ultimateModelListName: {ultimateModelListNames[i]}\")\n",
    "    for k in range(len(X_test)):  # Loop through test instances\n",
    "        \n",
    "\n",
    "        instance = X_test[k].reshape(1, -1)  # Ensure it's in the correct shape for prediction\n",
    "        prediction = model.predict(instance)[0]  # Get the predicted value\n",
    "        \n",
    "        if prediction == 1:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Event {k}: ModelName: {model_name} ModelPrediction: ----{prediction}--- X_TEST: {X_test[k]}\")\n",
    "        \n",
    "        # Ensure correct formatting for SHAP Explanation\n",
    "        explanation = shap.Explanation(\n",
    "            values=shap_values[k], \n",
    "            base_values=shap_values.base_values[k], \n",
    "            data=X_test[k].tolist() if isinstance(X_test[k], (np.ndarray, list)) else X_test[k],\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "\n",
    "        # Create and display the waterfall plot\n",
    "        destinationFolder = f\"{model_save_dir}/{model_name}/Pred_{prediction}\"\n",
    "        Utils.makeDirectoryIfNeededAt(destinationFolder)\n",
    "        \n",
    "        shap_waterall_plot_path = os.path.join(destinationFolder, f\"{k}_Pred{prediction}.png\")\n",
    "        # fig = plt.gcf()  # Get the current figure after the summary plot has been created\n",
    "        # fig.set_size_inches(6, 5)  \n",
    "        plt.figure(figsize=(6, 5))  \n",
    "        shap.waterfall_plot(explanation, show=False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(shap_waterall_plot_path)\n",
    "        # plt.savefig(shap_waterall_plot_path, bbox_inches='tight',pad_inches = 0.3)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SuspensionAF Attack Avg BC value: 0.112997 and AttackFree avg BC value: 0.127772\n",
      "Model: InjectionAF Attack Avg BC value: 0.095382 and AttackFree avg BC value: 0.128432\n",
      "Model: MasqueradeAF Attack Avg BC value: 0.164514 and AttackFree avg BC value: 0.127981\n",
      "Model: MixedAF Attack Avg BC value: 0.097734 and AttackFree avg BC value: 0.128514\n"
     ]
    }
   ],
   "source": [
    "#Betweenness Centrality average generation....\n",
    "import time\n",
    "\n",
    "uAttackBCVal = 0.0\n",
    "uAttackFreeBCVal = 0.0\n",
    "\n",
    "for i in range(0,len(modelNames)):\n",
    "    \n",
    "    attackBCValues = 0.0\n",
    "    attackFreeBCValues = 0.0\n",
    "    \n",
    "    model_name = modelNames[i]\n",
    "    shap_values = ultimateShapValues[i]\n",
    "    X_test = ultimateX_test[i]\n",
    "    model = ultimateModelList[i]\n",
    "    # print(f\"Event: MODEL NAME: {model_name}\")\n",
    "    attackCnt = 0\n",
    "    attackFreeCnt = 0\n",
    "    for k in range(0,len(X_test)):  # Loop through test instances\n",
    "        \n",
    "\n",
    "        instance = X_test[k].reshape(1, -1)  # Ensure it's in the correct shape for prediction\n",
    "        prediction = model.predict(instance)[0]  # Get the predicted value\n",
    "        \n",
    "        if prediction == 1:\n",
    "            attackFreeBCValues = attackFreeBCValues + instance[0,2]\n",
    "        else:\n",
    "            attackBCValues = attackBCValues + instance[0,2]\n",
    "        \n",
    "        # print(f\"Instance: {instance} Len: {len(instance)}\")\n",
    "    \n",
    "    totalCnt = len(X_test)\n",
    "    avgAttackBCValue = attackBCValues / totalCnt\n",
    "    avgAttackFreeBCValue = attackFreeBCValues / totalCnt\n",
    "    print(f\"Model: {model_name} Attack Avg BC value: {avgAttackBCValue:.6f} and AttackFree avg BC value: {avgAttackFreeBCValue:.6f}\")  \n",
    "    \n",
    "    uAttackBCVal = uAttackBCVal + avgAttackBCValue\n",
    "    uAttackFreeBCVal = uAttackFreeBCVal + avgAttackFreeBCValue\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultimate attack BC value: 0.117657 and attackFree BC value: 0.128175\n"
     ]
    }
   ],
   "source": [
    "uAttackBCVal = uAttackBCVal / 4.0\n",
    "uAttackFreeBCVal = uAttackFreeBCVal / 4.0\n",
    "print(f\"Ultimate attack BC value: {uAttackBCVal:.6f} and attackFree BC value: {uAttackFreeBCVal:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
